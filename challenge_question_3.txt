I have used a python module, food network wrapper, to scrape recipes for different cuisines, so far including mexican, polish, italian, and others. So far, I've scraped about 30 MB of this data in csv form. I created a set of stop words by looking at the ingredient lists for many different recipes across the different classes of foods. I eliminated these words and in the process tokenized the text. Using a small subset of this processed data (3.3 MB), I created key value pairs of occurences for ingredients, such as {cilantro leaves: 54} or {cinnamon: 85}. I generated word clouds and bar graphs for each of the different cuisines, including or omitting three spices that I have found are widely used across ethnic cuisines - salt, garlic, and pepper.

The word clouds display the findings of this analysis best, and these figures are available through the urls. With the three spices, the different cuisine word clouds look more similar. Eliminating these spices draws out the differences better so that there is less overlap in key ingredients used. With the data I have analyzed so far, one insight the word clouds may have is that the spices used in a cuisine are important characteristics, though some are shared across many cuisines. 

According to the data, French and Polish dishes tend to contain ingredients contained in baked goods and dairy products. French dishes tend on the sweet side while Polish dishes contain ingredients like sauerkraut and onions. Greek and Italian dishes are similar, though there might be a slight emphasis on citrus ingredients in Greek cuisine that is not as present in Italian. Japanese, Korean, Thai, Chinese, and Vietnamese food share ingredients not present in other dishes, including scallions and soy sauce. However, Vietnamese food tends more towards sweetness, Korean food emphasises sesame oil, Chinese food ginger (as does Indian and Japanese food), and Thai food lime juice and cilantro leaves. As suggested on the Data Incubator blog, it would be interesting to see if machine learning algorithms can pick up on these similarities in unlabelled data especially as more data and more features are added.

The preparation of the food is probably going to be important, so analyzing the text of the directions using something like tf-idf in the recipe is a next step. It will also be interesting to do topic modeling on the corpus to determine whether the classes determined using only features (ingredients) by an algorithm like latent dirichlet allocation will show similar wordclouds as those obtained using labeled recipes obtained from search results. Comparing the two kinds of analyses could give us a better idea of whether LDA or other methods are capturing something interesting about differences across food types.

Using machine learning algorithms like LDA, K-means, random forests, and dimensionality reduction are next steps in the analysis of this data set. As I have more time, I will make the data set larger and add other cuisines. This analysis could be useful for businesses in determining which ingredients correspond to popular dishes, which are more niche, and which are less popular overall region by region. Dimensionality reduction, classification, and clustering algorithms could also be useful in determining categories of food.
